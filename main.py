import urllib.request
from urllib.parse import urlparse
import re  # æ­£åˆ™
import os
from datetime import datetime, timedelta, timezone
import random
import opencc  # ç®€ç¹è½¬æ¢

# æ‰§è¡Œå¼€å§‹æ—¶é—´
timestart = datetime.now()

# è¯»å–æ–‡æœ¬æ–¹æ³•
def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines]
            return lines
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
        return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

# read BlackList 2024-06-17 15:02
def read_blacklist_from_txt(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    BlackList = [line.split(',')[1].strip() for line in lines if ',' in line]
    return BlackList

blacklist_auto = read_blacklist_from_txt(
    'assets/whitelist-blacklist/blacklist_auto.txt')
blacklist_manual = read_blacklist_from_txt(
    'assets/whitelist-blacklist/blacklist_manual.txt')
combined_blacklist = set(blacklist_auto + blacklist_manual)

# å®šä¹‰å¤šä¸ªå¯¹è±¡ç”¨äºå­˜å‚¨ä¸åŒå†…å®¹çš„è¡Œæ–‡æœ¬
# ä¸»é¢‘é“
ys_lines = []  # å¤®è§†é¢‘é“
ws_lines = []  # å«è§†é¢‘é“
dy_lines = []  # ç”µå½±é¢‘é“
gat_lines = []  # æ¸¯æ¾³å°
gj_lines = []  # å›½é™…å°
zb_lines = []  # ç›´æ’­ä¸­å›½
gd_lines = []  # åœ°æ–¹å°-å¹¿ä¸œé¢‘é“
hain_lines = []  # åœ°æ–¹å°-æµ·å—é¢‘é“

other_lines = []  # å…¶ä»–
other_lines_url = []  # ä¸ºé™ä½otheræ–‡ä»¶å¤§å°ï¼Œå‰”é™¤é‡å¤urlæ·»åŠ 

# è¯»å–æ–‡æœ¬
# ä¸»é¢‘é“
ys_dictionary = read_txt_to_array('ä¸»é¢‘é“/å¤®è§†é¢‘é“.txt')
ws_dictionary = read_txt_to_array('ä¸»é¢‘é“/å«è§†é¢‘é“.txt')
dy_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç”µå½±.txt')
gat_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ¸¯æ¾³å°.txt')
gj_dictionary = read_txt_to_array('ä¸»é¢‘é“/å›½é™…å°.txt')
zb_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt')

# åœ°æ–¹å°
gd_dictionary = read_txt_to_array('åœ°æ–¹å°/å¹¿ä¸œé¢‘é“.txt')
hain_dictionary = read_txt_to_array('åœ°æ–¹å°/æµ·å—é¢‘é“.txt')

# è‡ªå®šä¹‰æº
urls = read_txt_to_array('assets/urls.txt')

# ç®€ç¹è½¬æ¢
def traditional_to_simplified(text: str) -> str:
    # åˆå§‹åŒ–è½¬æ¢å™¨ï¼Œ"t2s" è¡¨ç¤ºä»ç¹ä½“è½¬ä¸ºç®€ä½“
    converter = opencc.OpenCC('t2s')
    simplified_text = converter.convert(text)
    return simplified_text

# M3Uæ ¼å¼åˆ¤æ–­
def is_m3u_content(text):
    lines = text.splitlines()
    first_line = lines[0].strip()
    return first_line.startswith("#EXTM3U")

def convert_m3u_to_txt(m3u_content):
    # åˆ†è¡Œå¤„ç†
    lines = m3u_content.split('\n')

    # ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨
    txt_lines = []

    # ä¸´æ—¶å˜é‡ç”¨äºå­˜å‚¨é¢‘é“åç§°
    channel_name = ""

    for line in lines:
        # è¿‡æ»¤æ‰ #EXTM3U å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTM3U"):
            continue
        # å¤„ç† #EXTINF å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTINF"):
            # è·å–é¢‘é“åç§°ï¼ˆå‡è®¾é¢‘é“åç§°åœ¨å¼•å·åï¼‰
            channel_name = line.split(',')[-1].strip()
        # å¤„ç† URL è¡Œ
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p"):
            txt_lines.append(f"{channel_name},{line.strip()}")

        # å¤„ç†åç¼€åä¸ºm3uï¼Œä½†æ˜¯å†…å®¹ä¸ºtxtçš„æ–‡ä»¶
        if "#genre#" not in line and "," in line and "://" in line:
            # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…é¢‘é“åç§°,URL çš„æ ¼å¼ï¼Œå¹¶ç¡®ä¿ URL åŒ…å« "://"
            # xxxx,http://xxxxx.xx.xx
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)

    # å°†ç»“æœåˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥æ¢è¡Œç¬¦åˆ†éš”
    return '\n'.join(txt_lines)

# åœ¨listæ˜¯å¦å·²ç»å­˜åœ¨url 2024-07-22 11:18
def check_url_existence(data_list, url):
    """
    Check if a given URL exists in a list of data.

    :param data_list: List of strings containing the data
    :param url: The URL to check for existence
    :return: True if the URL exists in the list, otherwise False
    """
    if "127.0.0.1" in url:
        return False
    # Extract URLs from the data list
    urls = [item.split(',')[1] for item in data_list]
    return url not in urls  # å¦‚æœä¸å­˜åœ¨åˆ™è¿”å›trueï¼Œéœ€è¦

# å¤„ç†å¸¦$çš„URLï¼ŒæŠŠ$ä¹‹åçš„å†…å®¹éƒ½å»æ‰ï¼ˆåŒ…æ‹¬$ä¹Ÿå»æ‰ï¼‰ ã€2024-08-08 22:29:11ã€‘
def clean_url(url):
    last_dollar_index = url.rfind('$')  # å®‰å…¨èµ·è§æ‰¾æœ€åä¸€ä¸ª$å¤„ç†
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

# æ·»åŠ channel_nameå‰å‰”é™¤éƒ¨åˆ†ç‰¹å®šå­—ç¬¦
removal_list = ["ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[ipv6]", "[ipv4]", "_ç”µä¿¡", "ç”µä¿¡", "ï¼ˆHDï¼‰", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", "-HD", "(HK)", "AKtv", "@", "IPV6", "ğŸï¸", "ğŸ¦", " ", "[BD]", "[VGA]", "[HD]", "[SD]", "(1080p)", "(720p)", "(480p)"]

def clean_channel_name(channel_name, removal_list):
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    channel_name = channel_name.replace("CCTV-", "CCTV")
    channel_name = channel_name.replace("CCTV0", "CCTV")
    channel_name = channel_name.replace("PLUS", "+")
    channel_name = channel_name.replace("NewTV-", "NewTV")
    channel_name = channel_name.replace("iHOT-", "iHOT")
    channel_name = channel_name.replace("NEW", "New")
    channel_name = channel_name.replace("New_", "New")
    return channel_name

# è¯»å–çº é”™é¢‘é“åç§°æ–¹æ³•
def load_corrections_name(filename):
    corrections = {}
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():  # è·³è¿‡ç©ºè¡Œ
                continue
            parts = line.strip().split(',')
            correct_name = parts[0]
            for name in parts[1:]:
                corrections[name] = correct_name
    return corrections

# è¯»å–çº é”™æ–‡ä»¶
corrections_name = load_corrections_name('assets/corrections_name.txt')

def correct_name_data(name):
    if name in corrections_name and name != corrections_name[name]:
        name = corrections_name[name]
    return name

# æ£€æŸ¥é¢‘é“æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§æºæ•°é‡
def is_channel_full(channel_name, target_list):
    """
    æ£€æŸ¥æŒ‡å®šé¢‘é“æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§æºæ•°é‡
    """
    # è®¡ç®—å½“å‰åˆ—è¡¨ä¸­è¯¥é¢‘é“çš„æºæ•°é‡
    count = sum(1 for line in target_list if line.startswith(channel_name + ","))
    return count >= 10  # æ¯ä¸ªé¢‘é“æœ€å¤š10æ¡æº

# åˆ†å‘ç›´æ’­æºï¼Œå½’ç±»ï¼ŒæŠŠè¿™éƒ¨åˆ†ä»process_urlå‰¥ç¦»å‡ºæ¥ï¼Œä¸ºä»¥ååŠ å…¥whitelistæºæ¸…å•åšå‡†å¤‡ã€‚
def process_channel_line(line):
    if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        channel_name = line.split(',')[0]
        channel_name = traditional_to_simplified(channel_name)  # ç¹è½¬ç®€
        # åˆ†å‘å‰æ¸…ç†channel_nameä¸­ç‰¹å®šå­—ç¬¦
        channel_name = clean_channel_name(channel_name, removal_list)
        channel_name = correct_name_data(channel_name).strip()  # æ ¹æ®çº é”™æ–‡ä»¶å¤„ç†

        # æŠŠURLä¸­$ä¹‹åçš„å†…å®¹éƒ½å»æ‰
        channel_address = clean_url(line.split(',')[1]).strip()
        line = channel_name+","+channel_address  # é‡æ–°ç»„ç»‡line

        if len(channel_address) > 0 and channel_address not in combined_blacklist:  # åˆ¤æ–­å½“å‰æºæ˜¯å¦åœ¨blacklistä¸­
            # æ ¹æ®è¡Œå†…å®¹åˆ¤æ–­å­˜å…¥å“ªä¸ªå¯¹è±¡ï¼Œå¼€å§‹åˆ†å‘
            if channel_name in ys_dictionary:  # å¤®è§†é¢‘é“
                if check_url_existence(ys_lines, channel_address) and not is_channel_full(channel_name, ys_lines):
                    ys_lines.append(line)
            elif channel_name in ws_dictionary:  # å«è§†é¢‘é“
                if check_url_existence(ws_lines, channel_address) and not is_channel_full(channel_name, ws_lines):
                    ws_lines.append(line)
            elif channel_name in dy_dictionary:  # ç”µå½±é¢‘é“
                if check_url_existence(dy_lines, channel_address) and not is_channel_full(channel_name, dy_lines):
                    dy_lines.append(line)
            elif channel_name in gat_dictionary:  # æ¸¯æ¾³å°
                if check_url_existence(gat_lines, channel_address) and not is_channel_full(channel_name, gat_lines):
                    gat_lines.append(line)
            elif channel_name in gj_dictionary:  # å›½é™…å°
                if check_url_existence(gj_lines, channel_address) and not is_channel_full(channel_name, gj_lines):
                    gj_lines.append(line)
            elif channel_name in zb_dictionary:  # ç›´æ’­ä¸­å›½
                if check_url_existence(zb_lines, channel_address) and not is_channel_full(channel_name, zb_lines):
                    zb_lines.append(line)
            elif channel_name in gd_dictionary:  # åœ°æ–¹å°-å¹¿ä¸œé¢‘é“
                if check_url_existence(gd_lines, channel_address) and not is_channel_full(channel_name, gd_lines):
                    gd_lines.append(line)
            elif channel_name in hain_dictionary:  # åœ°æ–¹å°-æµ·å—é¢‘é“
                if check_url_existence(hain_lines, channel_address) and not is_channel_full(channel_name, hain_lines):
                    hain_lines.append(line)
            else:
                if channel_address not in other_lines_url:
                    other_lines_url.append(channel_address)  # è®°å½•å·²åŠ url
                    other_lines.append(line)

def process_url(url):
    print(f"å¤„ç†URL: {url}")
    try:
        # åˆ›å»ºä¸€ä¸ªè¯·æ±‚å¯¹è±¡å¹¶æ·»åŠ è‡ªå®šä¹‰header
        headers = {
            'User-Agent': 'PostmanRuntime-ApipostRuntime/1.1.0',
        }
        req = urllib.request.Request(url, headers=headers)
        # æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
        with urllib.request.urlopen(req, timeout=10) as response:
            # ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–æ•°æ®
            data = response.read()
            # å°†äºŒè¿›åˆ¶æ•°æ®è§£ç ä¸ºå­—ç¬¦ä¸²
            try:
                # å…ˆå°è¯• UTF-8 è§£ç 
                text = data.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    # è‹¥ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯• GBK è§£ç 
                    text = data.decode('gbk')
                except UnicodeDecodeError:
                    try:
                        # è‹¥ GBK è§£ç å¤±è´¥ï¼Œå°è¯• ISO-8859-1 è§£ç 
                        text = data.decode('iso-8859-1')
                    except UnicodeDecodeError:
                        print("æ— æ³•ç¡®å®šåˆé€‚çš„ç¼–ç æ ¼å¼è¿›è¡Œè§£ç ã€‚")

            # å¤„ç†m3uæå–channel_nameå’Œchannel_address
            if is_m3u_content(text):
                text = convert_m3u_to_txt(text)

            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")
            for line in lines:
                if "#genre#" not in line and "," in line and "://" in line:
                    # æ‹†åˆ†æˆé¢‘é“åå’ŒURLéƒ¨åˆ†
                    channel_name, channel_address = line.split(',', 1)
                    # éœ€è¦åŠ å¤„ç†å¸¦#å·æº=äºˆåŠ é€Ÿæº
                    if "#" not in channel_address:
                        # å¦‚æœæ²¡æœ‰äº•å·ï¼Œåˆ™ç…§å¸¸æŒ‰ç…§æ¯è¡Œè§„åˆ™è¿›è¡Œåˆ†å‘
                        process_channel_line(line)
                    else:
                        # å¦‚æœæœ‰"#"å·ï¼Œåˆ™æ ¹æ®"#"å·åˆ†éš”
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline = f'{channel_name},{channel_url}'
                            process_channel_line(newline)

            # æ¯ä¸ªurlå¤„ç†å®Œæˆåï¼Œåœ¨other_linesåŠ ä¸ªå›è½¦ 2024-08-02 10:46
            other_lines.append('\n')

    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def sort_data(order, data):
    # åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ¯è¡Œæ•°æ®çš„ç´¢å¼•
    order_dict = {name: i for i, name in enumerate(order)}

    # å®šä¹‰ä¸€ä¸ªæ’åºé”®å‡½æ•°ï¼Œå¤„ç†ä¸åœ¨ order_dict ä¸­çš„å­—ç¬¦ä¸²
    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))

    # æŒ‰ç…§ order ä¸­çš„é¡ºåºå¯¹æ•°æ®è¿›è¡Œæ’åº
    sorted_data = sorted(data, key=sort_key)
    return sorted_data

# åŠ å…¥é…ç½®çš„url
for url in urls:
    if url.startswith("http"):
        process_url(url)

# è·å–å½“å‰çš„ UTC æ—¶é—´
utc_time = datetime.now(timezone.utc)
# åŒ—äº¬æ—¶é—´
beijing_time = utc_time + timedelta(hours=8)
# æ ¼å¼åŒ–ä¸ºæ‰€éœ€çš„æ ¼å¼
formatted_time = beijing_time.strftime("%Y%m%d %H:%M")
version = formatted_time + ",https://www.cloudplains.cn/tv202303.txt"

# åˆå¹¶æ‰€æœ‰å¯¹è±¡ä¸­çš„è¡Œæ–‡æœ¬ï¼ˆå»é‡ï¼Œæ’åºåæ‹¼æ¥ï¼‰
all_lines = ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n'] + \
           ["å¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, ys_lines) + ['\n'] + \
           ["å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, ws_lines) + ['\n'] + \
           ["æ¸¯æ¾³å°,#genre#"] + sort_data(gat_dictionary, gat_lines) + ['\n'] + \
           ["å›½é™…å°,#genre#"] + sort_data(gj_dictionary, gj_lines) + ['\n'] + \
           ["å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(gd_dictionary, gd_lines) + ['\n'] + \
           ["æµ·å—é¢‘é“,#genre#"] + sort_data(hain_dictionary, hain_lines) + ['\n'] + \
           ["ç”µå½±é¢‘é“,#genre#"] + sort_data(dy_dictionary, dy_lines) + ['\n'] + \
           ["ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zb_dictionary, zb_lines) + ['\n'] + \
           other_lines

# å°†åˆå¹¶åçš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
output_file = "live.txt"
# æœªåŒ¹é…çš„å†™å…¥æ–‡ä»¶
others_file = "others.txt"

try:
    # å…¨é›†ç‰ˆ
    with open(output_file, 'w', encoding='utf-8') as f:
        for line in all_lines:
            f.write(line + '\n')
    print(f"åˆå¹¶åçš„æ–‡æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")

    # å…¶ä»–
    with open(others_file, 'w', encoding='utf-8') as f:
        for line in other_lines:
            f.write(line + '\n')
    print(f"å…¶ä»–å·²ä¿å­˜åˆ°æ–‡ä»¶: {others_file}")

except Exception as e:
    print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def make_m3u(txt_file, m3u_file):
    try:
        output_text = '#EXTM3U x-tvg-url="https://epg.112114.xyz/pp.xml.gz"\n'
        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()

        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url = "https://epg.112114.xyz/logo/"+channel_name+".png"
                if logo_url is None:  # not found logo
                    output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
                else:
                    output_text += f"#EXTINF:-1  tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\"  group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"

        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)
        print(f"M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

make_m3u(output_file, "live.m3u")

# æ‰§è¡Œç»“æŸæ—¶é—´
timeend = datetime.now()

# è®¡ç®—æ—¶é—´å·®
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()

# è½¬æ¢ä¸ºåˆ†é’Ÿå’Œç§’
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)

print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")

combined_blacklist_hj = len(combined_blacklist)
all_lines_hj = len(all_lines)
other_lines_hj = len(other_lines)
print(f"blacklistè¡Œæ•°: {combined_blacklist_hj} ")
print(f"live.txtè¡Œæ•°: {all_lines_hj} ")
print(f"others.txtè¡Œæ•°: {other_lines_hj} ")

# å¤‡ç”¨1ï¼šhttp://tonkiang.us
# å¤‡ç”¨2ï¼šhttps://www.zoomeye.hk,https://www.shodan.io,https://tv.cctv.com/live/
# å¤‡ç”¨3ï¼š(BlackListæ£€æµ‹å¯¹è±¡)http,rtmp,p3p,rtpï¼ˆrtspï¼Œp2pï¼‰