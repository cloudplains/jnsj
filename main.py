import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import opencc
import ssl
import sys

# è·³è¿‡SSLè¯ä¹¦éªŒè¯
ssl._create_default_https_context = ssl._create_unverified_context

# æ‰§è¡Œå¼€å§‹æ—¶é—´
timestart = datetime.now()

# è®¾ç½®æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºç«‹å³åˆ·æ–°
sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 1)
sys.stderr = os.fdopen(sys.stderr.fileno(), 'w', 1)

print("è„šæœ¬å¼€å§‹æ‰§è¡Œ")

# è¯»å–æ–‡æœ¬æ–¹æ³•ï¼ˆæ”¯æŒå¤šç§ç¼–ç ï¼‰
def read_txt_to_array(file_name):
    try:
        # å…ˆå°è¯• UTF-8 ç¼–ç 
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines]
            return lines
    except UnicodeDecodeError:
        try:
            # å¦‚æœ UTF-8 å¤±è´¥ï¼Œå°è¯• GBK ç¼–ç 
            with open(file_name, 'r', encoding='gbk') as file:
                lines = file.readlines()
                lines = [line.strip() for line in lines]
                return lines
        except UnicodeDecodeError:
            try:
                # å¦‚æœ GBK ä¹Ÿå¤±è´¥ï¼Œå°è¯• latin-1 ç¼–ç 
                with open(file_name, 'r', encoding='latin-1') as file:
                    lines = file.readlines()
                    lines = [line.strip() for line in lines]
                    return lines
            except Exception as e:
                print(f"æ— æ³•ç¡®å®šåˆé€‚çš„ç¼–ç æ ¼å¼è¿›è¡Œè§£ç æ–‡ä»¶: {file_name}, é”™è¯¯: {e}")
                return []
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
        return []
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

# è¯»å–é»‘åå•ï¼ˆæ”¯æŒå¤šç§ç¼–ç ï¼‰
def read_blacklist_from_txt(file_path):
    try:
        # å…ˆå°è¯• UTF-8 ç¼–ç 
        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
    except UnicodeDecodeError:
        try:
            # å¦‚æœ UTF-8 å¤±è´¥ï¼Œå°è¯• GBK ç¼–ç 
            with open(file_path, 'r', encoding='gbk') as file:
                lines = file.readlines()
        except UnicodeDecodeError:
            try:
                # å¦‚æœ GBK ä¹Ÿå¤±è´¥ï¼Œå°è¯• latin-1 ç¼–ç 
                with open(file_path, 'r', encoding='latin-1') as file:
                    lines = file.readlines()
            except Exception as e:
                print(f"è¯»å–é»‘åå•æ—¶å‡ºé”™: {e}")
                return []

    BlackList = [line.split(',')[1].strip() for line in lines if ',' in line]
    return BlackList

print("æ­£åœ¨è¯»å–é»‘åå•...")
blacklist_auto = read_blacklist_from_txt('assets/whitelist-blacklist/blacklist_auto.txt')
blacklist_manual = read_blacklist_from_txt('assets/whitelist-blacklist/blacklist_manual.txt')
combined_blacklist = set(blacklist_auto + blacklist_manual)
print(f"åˆå¹¶é»‘åå•è¡Œæ•°: {len(combined_blacklist)}")

# å®šä¹‰å¤šä¸ªå¯¹è±¡ç”¨äºå­˜å‚¨ä¸åŒå†…å®¹çš„è¡Œæ–‡æœ¬
ys_lines = []  # å¤®è§†é¢‘é“
ws_lines = []  # å«è§†é¢‘é“
dy_lines = []  # ç”µå½±é¢‘é“
gat_lines = []  # æ¸¯æ¾³å°
gj_lines = []  # å›½é™…å°
zb_lines = []  # ç›´æ’­ä¸­å›½
gd_lines = []  # åœ°æ–¹å°-å¹¿ä¸œé¢‘é“
hain_lines = []  # åœ°æ–¹å°-æµ·å—é¢‘é“

other_lines = []  # å…¶ä»–
other_lines_url = []  # ä¸ºé™ä½otheræ–‡ä»¶å¤§å°ï¼Œå‰”é™¤é‡å¤urlæ·»åŠ 

print("æ­£åœ¨è¯»å–é¢‘é“å­—å…¸...")
# è¯»å–æ–‡æœ¬
# ä¸»é¢‘é“
ys_dictionary = read_txt_to_array('ä¸»é¢‘é“/å¤®è§†é¢‘é“.txt')
ws_dictionary = read_txt_to_array('ä¸»é¢‘é“/å«è§†é¢‘é“.txt')
dy_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç”µå½±.txt')
gat_dictionary = read_txt_to_array('ä¸»é¢‘é“/æ¸¯æ¾³å°.txt')
gj_dictionary = read_txt_to_array('ä¸»é¢‘é“/å›½é™…å°.txt')
zb_dictionary = read_txt_to_array('ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt')

# åœ°æ–¹å°
gd_dictionary = read_txt_to_array('åœ°æ–¹å°/å¹¿ä¸œé¢‘é“.txt')
hain_dictionary = read_txt_to_array('åœ°æ–¹å°/æµ·å—é¢‘é“.txt')

# è‡ªå®šä¹‰æº
urls = read_txt_to_array('assets/urls.txt')
print(f"è¯»å–åˆ° {len(urls)} ä¸ªURL")

# ç®€ç¹è½¬æ¢
def traditional_to_simplified(text: str) -> str:
    try:
        converter = opencc.OpenCC('t2s')
        simplified_text = converter.convert(text)
        return simplified_text
    except Exception as e:
        print(f"ç¹ç®€è½¬æ¢é”™è¯¯: {e}")
        return text

# M3Uæ ¼å¼åˆ¤æ–­
def is_m3u_content(text):
    if not text:
        return False
    lines = text.splitlines()
    if not lines:
        return False
    first_line = lines[0].strip()
    return first_line.startswith("#EXTM3U")

def convert_m3u_to_txt(m3u_content):
    # åˆ†è¡Œå¤„ç†
    lines = m3u_content.split('\n')

    # ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨
    txt_lines = []

    # ä¸´æ—¶å˜é‡ç”¨äºå­˜å‚¨é¢‘é“åç§°
    channel_name = ""

    for line in lines:
        # è¿‡æ»¤æ‰ #EXTM3U å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTM3U"):
            continue
        # å¤„ç† #EXTINF å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTINF"):
            # è·å–é¢‘é“åç§°ï¼ˆå‡è®¾é¢‘é“åç§°åœ¨å¼•å·åï¼‰
            channel_name = line.split(',')[-1].strip()
        # å¤„ç† URL è¡Œ
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p"):
            txt_lines.append(f"{channel_name},{line.strip()}")

        # å¤„ç†åç¼€åä¸ºm3uï¼Œä½†æ˜¯å†…å®¹ä¸ºtxtçš„æ–‡ä»¶
        if "#genre#" not in line and "," in line and "://" in line:
            # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…é¢‘é“åç§°,URL çš„æ ¼å¼ï¼Œå¹¶ç¡®ä¿ URL åŒ…å« "://"
            # xxxx,http://xxxxx.xx.xx
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)

    # å°†ç»“æœåˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥æ¢è¡Œç¬¦åˆ†éš”
    return '\n'.join(txt_lines)

# åœ¨listæ˜¯å¦å·²ç»å­˜åœ¨url
def check_url_existence(data_list, url):
    if "127.0.0.1" in url:
        return False
    # Extract URLs from the data list
    urls = [item.split(',')[1] for item in data_list if len(item.split(',')) > 1]
    return url not in urls  # å¦‚æœä¸å­˜åœ¨åˆ™è¿”å›trueï¼Œéœ€è¦

# å¤„ç†å¸¦$çš„URL
def clean_url(url):
    last_dollar_index = url.rfind('$')
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

# æ·»åŠ channel_nameå‰å‰”é™¤éƒ¨åˆ†ç‰¹å®šå­—ç¬¦
removal_list = ["ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[ipv6]", "[ipv4]", "_ç”µä¿¡", "ç”µä¿¡", "ï¼ˆHDï¼‰", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", "-HD", "(HK)", "AKtv", "@", "IPV6", "ğŸï¸", "ğŸ¦", " ", "[BD]", "[VGA]", "[HD]", "[SD]", "(1080p)", "(720p)", "(480p)"]

def clean_channel_name(channel_name, removal_list):
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    channel_name = channel_name.replace("CCTV-", "CCTV")
    channel_name = channel_name.replace("CCTV0", "CCTV")
    channel_name = channel_name.replace("PLUS", "+")
    channel_name = channel_name.replace("NewTV-", "NewTV")
    channel_name = channel_name.replace("iHOT-", "iHOT")
    channel_name = channel_name.replace("NEW", "New")
    channel_name = channel_name.replace("New_", "New")
    return channel_name

# è¯»å–çº é”™é¢‘é“åç§°æ–¹æ³•
def load_corrections_name(filename):
    corrections = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                parts = line.strip().split(',')
                if len(parts) < 2:
                    continue
                correct_name = parts[0]
                for name in parts[1:]:
                    corrections[name] = correct_name
    except Exception as e:
        print(f"è¯»å–çº é”™æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    return corrections

# è¯»å–çº é”™æ–‡ä»¶
corrections_name = load_corrections_name('assets/corrections_name.txt')

def correct_name_data(name):
    if name in corrections_name and name != corrections_name[name]:
        name = corrections_name[name]
    return name

# æ£€æŸ¥é¢‘é“æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§æºæ•°é‡
def is_channel_full(channel_name, target_list):
    count = sum(1 for line in target_list if line.startswith(channel_name + ","))
    return count >= 10

# åˆ†å‘ç›´æ’­æº
def process_channel_line(line):
    try:
        if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
            parts = line.split(',', 1)
            if len(parts) < 2:
                return
                
            channel_name = parts[0]
            channel_name = traditional_to_simplified(channel_name)
            channel_name = clean_channel_name(channel_name, removal_list)
            channel_name = correct_name_data(channel_name).strip()

            channel_address = clean_url(parts[1]).strip()
            line = channel_name + "," + channel_address

            if len(channel_address) > 0 and channel_address not in combined_blacklist:
                # ç‰¹åˆ«å¤„ç†ç›´æ’­ä¸­å›½åˆ†ç±» - åªä¿ç•™æ˜ç¡®çš„ç›´æ’­ä¸­å›½é¢‘é“
                if channel_name in zb_dictionary:
                    if check_url_existence(zb_lines, channel_address) and not is_channel_full(channel_name, zb_lines):
                        zb_lines.append(line)
                elif channel_name in ys_dictionary:
                    if check_url_existence(ys_lines, channel_address) and not is_channel_full(channel_name, ys_lines):
                        ys_lines.append(line)
                elif channel_name in ws_dictionary:
                    if check_url_existence(ws_lines, channel_address) and not is_channel_full(channel_name, ws_lines):
                        ws_lines.append(line)
                elif channel_name in dy_dictionary:
                    if check_url_existence(dy_lines, channel_address) and not is_channel_full(channel_name, dy_lines):
                        dy_lines.append(line)
                elif channel_name in gat_dictionary:
                    if check_url_existence(gat_lines, channel_address) and not is_channel_full(channel_name, gat_lines):
                        gat_lines.append(line)
                elif channel_name in gj_dictionary:
                    if check_url_existence(gj_lines, channel_address) and not is_channel_full(channel_name, gj_lines):
                        gj_lines.append(line)
                elif channel_name in gd_dictionary:
                    if check_url_existence(gd_lines, channel_address) and not is_channel_full(channel_name, gd_lines):
                        gd_lines.append(line)
                elif channel_name in hain_dictionary:
                    if check_url_existence(hain_lines, channel_address) and not is_channel_full(channel_name, hain_lines):
                        hain_lines.append(line)
                else:
                    if channel_address not in other_lines_url:
                        other_lines_url.append(channel_address)
                        other_lines.append(line)
    except Exception as e:
        print(f"å¤„ç†é¢‘é“è¡Œæ—¶å‡ºé”™: {e}, è¡Œå†…å®¹: {line}")

def process_url(url):
    print(f"\nå¼€å§‹å¤„ç†URL: {url}")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=20) as response:
            data = response.read()
            try:
                text = data.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    text = data.decode('gbk')
                except UnicodeDecodeError:
                    try:
                        text = data.decode('iso-8859-1')
                    except UnicodeDecodeError:
                        print("æ— æ³•ç¡®å®šåˆé€‚çš„ç¼–ç æ ¼å¼è¿›è¡Œè§£ç ã€‚")
                        return

            if is_m3u_content(text):
                text = convert_m3u_to_txt(text)

            lines = text.split('\n')
            print(f"åŸå§‹è¡Œæ•°: {len(lines)}")
            valid_lines = 0
            for line in lines:
                if "#genre#" not in line and "," in line and "://" in line:
                    parts = line.split(',', 1)
                    if len(parts) < 2:
                        continue
                        
                    channel_name, channel_address = parts
                    if "#" not in channel_address:
                        process_channel_line(line)
                        valid_lines += 1
                    else:
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            if "://" in channel_url:
                                newline = f'{channel_name},{channel_url}'
                                process_channel_line(newline)
                                valid_lines += 1

            print(f"æœ‰æ•ˆè¡Œæ•°: {valid_lines}")
            other_lines.append('\n')

    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def sort_data(order, data):
    order_dict = {name: i for i, name in enumerate(order)}

    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))

    sorted_data = sorted(data, key=sort_key)
    return sorted_data

# åŠ å…¥é…ç½®çš„url
print("\nå¼€å§‹å¤„ç†æ‰€æœ‰URL...")
for url in urls:
    if url.startswith("http"):
        process_url(url)

# è·å–å½“å‰çš„ UTC æ—¶é—´
utc_time = datetime.now(timezone.utc)
beijing_time = utc_time + timedelta(hours=8)
formatted_time = beijing_time.strftime("%Y%m%d %H:%M")
version = formatted_time + ",https://www.cloudplains.cn/tv202303.txt"

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
print(f"\nç»Ÿè®¡ä¿¡æ¯:")
print(f"å¤®è§†é¢‘é“: {len(ys_lines)} è¡Œ")
print(f"å«è§†é¢‘é“: {len(ws_lines)} è¡Œ")
print(f"ç”µå½±é¢‘é“: {len(dy_lines)} è¡Œ")
print(f"æ¸¯æ¾³å°: {len(gat_lines)} è¡Œ")
print(f"å›½é™…å°: {len(gj_lines)} è¡Œ")
print(f"ç›´æ’­ä¸­å›½: {len(zb_lines)} è¡Œ")
print(f"å¹¿ä¸œé¢‘é“: {len(gd_lines)} è¡Œ")
print(f"æµ·å—é¢‘é“: {len(hain_lines)} è¡Œ")
print(f"å…¶ä»–: {len(other_lines)} è¡Œ")

# åˆå¹¶æ‰€æœ‰å¯¹è±¡ä¸­çš„è¡Œæ–‡æœ¬
all_lines = ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n'] + \
           ["å¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, ys_lines) + ['\n'] + \
           ["å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, ws_lines) + ['\n'] + \
           ["æ¸¯æ¾³å°,#genre#"] + sort_data(gat_dictionary, gat_lines) + ['\n'] + \
           ["å›½é™…å°,#genre#"] + sort_data(gj_dictionary, gj_lines) + ['\n'] + \
           ["å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(gd_dictionary, gd_lines) + ['\n'] + \
           ["æµ·å—é¢‘é“,#genre#"] + sort_data(hain_dictionary, hain_lines) + ['\n'] + \
           ["ç”µå½±é¢‘é“,#genre#"] + sort_data(dy_dictionary, dy_lines) + ['\n'] + \
           ["ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zb_dictionary, zb_lines) + ['\n'] + \
           other_lines

# å°†åˆå¹¶åçš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
output_file = "live.txt"

try:
    with open(output_file, 'w', encoding='utf-8') as f:
        for line in all_lines:
            f.write(line + '\n')
    print(f"åˆå¹¶åçš„æ–‡æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")

except Exception as e:
    print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def make_m3u(txt_file, m3u_file):
    try:
        output_text = '#EXTM3U x-tvg-url="https://epg.112114.xyz/pp.xml.gz"\n'
        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()

        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url = "https://epg.112114.xyz/logo/"+channel_name+".png"
                output_text += f"#EXTINF:-1 tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\" group-title=\"{group_name}\",{channel_name}\n"
                output_text += f"{channel_url}\n"

        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)
        print(f"M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"ç”ŸæˆM3Uæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

make_m3u(output_file, "live.m3u")

# æ‰§è¡Œç»“æŸæ—¶é—´
timeend = datetime.now()
elapsed_time = timeend - timestart
total_seconds = elapsed_time.total_seconds()
minutes = int(total_seconds // 60)
seconds = int(total_seconds % 60)

print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")
print(f"blacklistè¡Œæ•°: {len(combined_blacklist)}")
print(f"live.txtè¡Œæ•°: {len(all_lines)}")